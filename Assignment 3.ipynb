{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a838376-af25-414a-a3c9-d56ab2c9cc0e",
   "metadata": {},
   "source": [
    "# Assignment 2 - 19/11/2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381dfe09-43a3-4308-b51d-f00bbfd6a761",
   "metadata": {},
   "source": [
    "**Objective:** Learn and apply web scraping techniques using Python to extract data from web pages, focusing on understanding and utilizing various tools and methods. \n",
    "**Scope:** This assignment will involve scraping a simple, legally permissible website (e.g., a webpage \n",
    "displaying weather data, book listings, or movie reviews). Ensure the chosen website allows scraping by checking its robots.txt file and terms of service. \n",
    "\n",
    "**Tasks:** \n",
    "1. Choose a Website:\n",
    "- Identify a website to scrape. Verify that the website permits scraping. \n",
    "- Briefly describe the website and the data you intend to scrape. \n",
    "3. Set Up Your Environment: \n",
    "- Install necessary Python libraries (requests, Scrapy, etc.). \n",
    "- Set up a Python script or Jupyter notebook for your scraping code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75ea8df7-c4ee-4624-90ed-37d44b28ad1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scrapy in c:\\users\\belki\\anaconda3\\lib\\site-packages (2.11.1)\n",
      "Requirement already satisfied: Twisted>=18.9.0 in c:\\users\\belki\\anaconda3\\lib\\site-packages (from scrapy) (23.10.0)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in c:\\users\\belki\\anaconda3\\lib\\site-packages (from scrapy) (42.0.5)\n",
      "Requirement already satisfied: cssselect>=0.9.1 in c:\\users\\belki\\anaconda3\\lib\\site-packages (from scrapy) (1.2.0)\n",
      "Requirement already satisfied: itemloaders>=1.0.1 in c:\\users\\belki\\anaconda3\\lib\\site-packages (from scrapy) (1.1.0)\n",
      "Requirement already satisfied: parsel>=1.5.0 in c:\\users\\belki\\anaconda3\\lib\\site-packages (from scrapy) (1.8.1)\n",
      "Requirement already satisfied: pyOpenSSL>=21.0.0 in c:\\users\\belki\\anaconda3\\lib\\site-packages (from scrapy) (24.0.0)\n",
      "Requirement already satisfied: queuelib>=1.4.2 in c:\\users\\belki\\anaconda3\\lib\\site-packages (from scrapy) (1.6.2)\n",
      "Requirement already satisfied: service-identity>=18.1.0 in c:\\users\\belki\\anaconda3\\lib\\site-packages (from scrapy) (18.1.0)\n",
      "Requirement already satisfied: w3lib>=1.17.0 in c:\\users\\belki\\anaconda3\\lib\\site-packages (from scrapy) (2.1.2)\n",
      "Requirement already satisfied: zope.interface>=5.1.0 in c:\\users\\belki\\anaconda3\\lib\\site-packages (from scrapy) (5.4.0)\n",
      "Requirement already satisfied: protego>=0.1.15 in c:\\users\\belki\\anaconda3\\lib\\site-packages (from scrapy) (0.1.16)\n",
      "Requirement already satisfied: itemadapter>=0.1.0 in c:\\users\\belki\\anaconda3\\lib\\site-packages (from scrapy) (0.3.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\belki\\anaconda3\\lib\\site-packages (from scrapy) (69.5.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\belki\\anaconda3\\lib\\site-packages (from scrapy) (23.2)\n",
      "Requirement already satisfied: tldextract in c:\\users\\belki\\anaconda3\\lib\\site-packages (from scrapy) (3.2.0)\n",
      "Requirement already satisfied: lxml>=4.4.1 in c:\\users\\belki\\anaconda3\\lib\\site-packages (from scrapy) (5.2.1)\n",
      "Requirement already satisfied: PyDispatcher>=2.0.5 in c:\\users\\belki\\anaconda3\\lib\\site-packages (from scrapy) (2.0.5)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\belki\\anaconda3\\lib\\site-packages (from cryptography>=36.0.0->scrapy) (1.16.0)\n",
      "Requirement already satisfied: jmespath>=0.9.5 in c:\\users\\belki\\anaconda3\\lib\\site-packages (from itemloaders>=1.0.1->scrapy) (1.0.1)\n",
      "Requirement already satisfied: six in c:\\users\\belki\\anaconda3\\lib\\site-packages (from protego>=0.1.15->scrapy) (1.16.0)\n",
      "Requirement already satisfied: attrs>=16.0.0 in c:\\users\\belki\\anaconda3\\lib\\site-packages (from service-identity>=18.1.0->scrapy) (23.1.0)\n",
      "Requirement already satisfied: pyasn1-modules in c:\\users\\belki\\anaconda3\\lib\\site-packages (from service-identity>=18.1.0->scrapy) (0.2.8)\n",
      "Requirement already satisfied: pyasn1 in c:\\users\\belki\\anaconda3\\lib\\site-packages (from service-identity>=18.1.0->scrapy) (0.4.8)\n",
      "Requirement already satisfied: automat>=0.8.0 in c:\\users\\belki\\anaconda3\\lib\\site-packages (from Twisted>=18.9.0->scrapy) (20.2.0)\n",
      "Requirement already satisfied: constantly>=15.1 in c:\\users\\belki\\anaconda3\\lib\\site-packages (from Twisted>=18.9.0->scrapy) (23.10.4)\n",
      "Requirement already satisfied: hyperlink>=17.1.1 in c:\\users\\belki\\anaconda3\\lib\\site-packages (from Twisted>=18.9.0->scrapy) (21.0.0)\n",
      "Requirement already satisfied: incremental>=22.10.0 in c:\\users\\belki\\anaconda3\\lib\\site-packages (from Twisted>=18.9.0->scrapy) (22.10.0)\n",
      "Requirement already satisfied: twisted-iocpsupport<2,>=1.0.2 in c:\\users\\belki\\anaconda3\\lib\\site-packages (from Twisted>=18.9.0->scrapy) (1.0.2)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\belki\\anaconda3\\lib\\site-packages (from Twisted>=18.9.0->scrapy) (4.11.0)\n",
      "Requirement already satisfied: idna in c:\\users\\belki\\anaconda3\\lib\\site-packages (from tldextract->scrapy) (3.7)\n",
      "Requirement already satisfied: requests>=2.1.0 in c:\\users\\belki\\anaconda3\\lib\\site-packages (from tldextract->scrapy) (2.32.2)\n",
      "Requirement already satisfied: requests-file>=1.4 in c:\\users\\belki\\anaconda3\\lib\\site-packages (from tldextract->scrapy) (1.5.1)\n",
      "Requirement already satisfied: filelock>=3.0.8 in c:\\users\\belki\\anaconda3\\lib\\site-packages (from tldextract->scrapy) (3.13.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\belki\\anaconda3\\lib\\site-packages (from cffi>=1.12->cryptography>=36.0.0->scrapy) (2.21)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\belki\\anaconda3\\lib\\site-packages (from requests>=2.1.0->tldextract->scrapy) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\belki\\anaconda3\\lib\\site-packages (from requests>=2.1.0->tldextract->scrapy) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\belki\\anaconda3\\lib\\site-packages (from requests>=2.1.0->tldextract->scrapy) (2024.7.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "5bc4f727-1b9e-4365-992c-79478d4b82e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of elements: 700\n"
     ]
    }
   ],
   "source": [
    "from scrapy import Selector\n",
    "import requests\n",
    "# I import a scrapy selector and requests\n",
    "titles=[]\n",
    "dates=[]\n",
    "rates=[]\n",
    "serves=[]\n",
    "prep_times=[]\n",
    "cook_times=[]\n",
    "recipes=[]\n",
    "#I create empty lists to put relevant info in it later.\n",
    "\n",
    "url = 'https://yemek.com/tarif/'\n",
    "#Then I define the url that I scrape\n",
    "html = requests.get(url).content\n",
    "# I create the string html containing the HTML source\n",
    "\n",
    "sel = Selector(text = html)\n",
    "# I create the selector object sel from html\n",
    "\n",
    "print(\"Number of elements:\", len(sel.xpath('//*')))\n",
    "# I check the number of elements in the html document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "f126df7e-1bb0-4363-aedd-d6c8b539186d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/tarif/mini-bal-kabagi-dolmasi/', '/tarif/palamut-eksilisi/', '/tarif/sinkonta/', '/tarif/ayva-dolmasi-2/', '/tarif/firinda-kereviz/', '/tarif/etsiz-kuru-fasulye/', '/tarif/perde-pilavi/', '/tarif/tarhana-corbasi/', '/tarif/firinda-palamut/']\n"
     ]
    }
   ],
   "source": [
    "xpath_for_recipe_links = '//*[@id=\"__next\"]/div/div[3]/main/section[2]/div[4]/div/div[1]/div/div/div/h4/a/@href'\n",
    "recipe_links = sel.xpath(xpath_for_recipe_links).extract()\n",
    "print(recipe_links)\n",
    "#After finding the page where I will get the recipes, I create a common xpath by taking the xpaths of several recipes and check if I could get them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfa411f-9b9e-45a3-8114-36720ab5a5b3",
   "metadata": {},
   "source": [
    "3. Data Extraction: \n",
    "- Use the requests library to send an HTTP request and retrieve the content of the \n",
    "webpage. \n",
    "- Parse the webpage content using Scrapy to extract relevant data (e.g., titles, \n",
    "descriptions, ratings). \n",
    "4. Data Cleaning: \n",
    "- Clean and organize the scraped data into a readable and usable format. \n",
    "- Handle any missing or inconsistent data entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "92b72cd1-90b9-4992-8e97-55f9fad1c4ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mini Bal Kabağı Dolması Tarifi', 'Palamut Ekşilisi Tarifi', 'Sinkonta Tarifi', 'Ayva Dolması Tarifi', 'Fırında Kereviz Tarifi', 'Etsiz Kuru Fasulye Tarifi', 'Perde Pilavı Tarifi', 'Tarhana Çorbası Tarifi', 'Fırında Palamut Tarifi']\n",
      "[['19 Ekim 2022'], ['19 Ekim 2022'], ['19 Ekim 2022'], ['19 Ekim 2022'], ['19 Ekim 2022'], ['19 Ekim 2022'], ['19 Ekim 2022'], ['19 Ekim 2022'], ['19 Ekim 2022']]\n",
      "[['5'], ['5'], ['4.5'], ['4.5'], ['4.5'], ['5'], ['5'], ['4.5'], ['4.5']]\n",
      "[['4 kişilik'], ['4 kişilik'], ['4 kişilik'], ['3 adet'], ['4 kişilik'], ['6 kişilik'], ['6 kişilik'], ['6 kişilik'], ['2 adet']]\n",
      "[['30 dakika'], ['10 dakika'], ['15 dakika'], ['20 dakika'], ['15 dakika'], ['30 dakika'], ['30 dakika'], ['5 dakika'], ['5 dakika']]\n",
      "[['50 dakika'], ['30 dakika'], ['25 dakika'], ['30 dakika'], ['45 dakika'], ['90 dakika'], ['1 saat'], ['20 dakika'], ['30 dakika']]\n",
      "[[], [], ['Acı yağ yakılmış süzme yoğurtla birlikte servis edebilirsiniz.  '], [], ['Kerevizleri doğradıktan sonra hemen tarife başlamayacaksanız limonla ovabilir ya da limonlu suda bekleterek kararmasını engelleyebilirsiniz.  '], [], ['Tavuk but ve göğüs etini üzerini geçecek kadar su ilavesiyle kısık ateşte haşlayın. Arzuya göre kuru soğan, defne yaprağı, kök sebzeler ve karanfil ilave edin. Haşlanmış tavukları deri ve kemik kısımlarını ayıkladıktan sonra didin.Pilavın daha zengin tatlara sahip olması için; haşlanmış göğüs eti yerine but ve göğüs etini bir arada kullanın. Bu işlem pilav yapımında kullandığınız tavuk suyunun daha yağlı ve lezzetli olmasını sağlayacaktır.Hamurun daha iyi kızarması için; pişirme kaplarını yağlama işleminde kullandığınız tereyağına çok az miktarda pekmez ekleyebilirsiniz.  '], ['Toz tarhanayı oda sıcaklığındaki su ile ocağa almadan karıştırın aksi halde topaklanma sorunu olabilir.  '], ['Balıkları önceden yıkamayı ve kanlarını tamamen süzmeyi unutmayın. Dilerseniz fırına vermeden önce, tereyağı da ekleyebilirsiniz.  ']]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "for recipe_link in recipe_links:\n",
    "    url1 = 'https://yemek.com' + recipe_link\n",
    "#Then I create a for loop to go to each recipe page to get the info that I want. Therefore I define the base url and add the suffix for each recipe.\n",
    "\n",
    "    html1 = requests.get(url1).content\n",
    "    sel1 = Selector(text = html1)\n",
    "    xpath_for_title = '//*[@id=\"__next\"]/div/div[3]/main/section[2]/div/div[1]/div[1]/div[1]/div/span/text()'\n",
    "    title = sel1.xpath(xpath_for_title).extract()\n",
    "    if len(title)==2:\n",
    "        if title[1] not in titles:\n",
    "            titles.append(title[1])\n",
    "    elif len(title)==1:\n",
    "        if title[0] not in titles:\n",
    "            titles.append(title[0])\n",
    "    else:\n",
    "        titles.append(np.nan)\n",
    "    #Since there is a blank column on my titles, I create an if function to solve it.\n",
    "    #If the length is 2 I get 1st, if it is 1 I get index 0, and in other cases, I mark it nan.\n",
    "    dates.append(date)\n",
    "    xpath_for_rate = '//*[@id=\"__next\"]/div/div[3]/main/section[2]/div/div[2]/div[1]/div/div[2]/span[2]/text()'\n",
    "    rate = sel1.xpath(xpath_for_rate).extract()\n",
    "    rates.append(rate)\n",
    "    xpath_for_serve = '//*[@id=\"__next\"]/div/div[3]/main/section[2]/div/div[1]/section/div[1]/div[1]/div/span/text()'\n",
    "    serve = sel1.xpath(xpath_for_serve).extract()\n",
    "    serves.append(serve)\n",
    "    xpath_for_prep_time = '//*[@id=\"__next\"]/div/div[3]/main/section[2]/div/div[1]/section/div[1]/div[2]/div/span/text()'\n",
    "    prep_time = sel1.xpath(xpath_for_prep_time).extract()\n",
    "    prep_times.append(prep_time)\n",
    "    xpath_for_cook_time = '//*[@id=\"__next\"]/div/div[3]/main/section[2]/div/div[1]/section/div[1]/div[3]/div/span/text()'\n",
    "    cook_time = sel1.xpath(xpath_for_cook_time).extract()\n",
    "    cook_times.append(cook_time)\n",
    "    xpath_for_recipe = '//*[@id=\"__next\"]/div/div[3]/main/section[2]/div/div[1]/section/div[4]/div[2]/text()'\n",
    "    recipe = sel1.xpath(xpath_for_recipe).extract()\n",
    "    recipes.append(recipe)\n",
    "    #Then I get some data about the recipe by defining their xpaths.\n",
    "\n",
    "print(titles)\n",
    "print(dates)\n",
    "print(rates)\n",
    "print(serves)\n",
    "print(prep_times)\n",
    "print(cook_times)\n",
    "print(recipes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996673a2-1d49-4053-a3ab-c91e23d84cf5",
   "metadata": {},
   "source": [
    "5. Data Storage: \n",
    "- Store the extracted data in a structured format like a CSV file or a Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "67b83fb4-a261-4a55-8353-dc6236aed5c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have done it\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "recipe_df = pd.DataFrame({\n",
    "    \"Recipe Name\": titles,\n",
    "    \"Recipe Date\": dates,\n",
    "    \"Serves\": serves,\n",
    "    \"Preparation Time\": prep_times,\n",
    "    \"Cooking Time\": cook_times,\n",
    "    \"Recipe\": recipes,\n",
    "    \n",
    "})\n",
    "#I make a pandas data frame by merging all my lists.\n",
    "\n",
    "recipe_df = recipe_df.fillna(np.nan)\n",
    "\n",
    "recipe_df.to_excel(\"recipes_output.xlsx\")\n",
    "print(\"You have done it\")\n",
    "#Lastly I write it on an xlsx file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f05ef1-9a24-46bf-ba86-4aacb6f1a9bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
